{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Paper.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "qxj6XAMSqzzH",
        "VOapXXZvlHf7",
        "FWh2CMp_CPTI",
        "mEHiGGeq_EjU",
        "J_3DXsxB_SI7",
        "PCqp-tkxDQIH",
        "-9_063erDNBj",
        "hNNqyJ8VFf_z",
        "jhhwrquM-953",
        "76W2TKdsGcQs",
        "cweX1vLhHBv1",
        "Nte_xassIpSD",
        "K1STMh_VI2o2",
        "OyQPX0ujI6WN",
        "M5POonlaTuxK",
        "x57Z0fcOTxGH",
        "lBPOFN21Tyx4",
        "Ft_msJe2T042",
        "hbfXYgPDT2jU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxj6XAMSqzzH",
        "colab_type": "text"
      },
      "source": [
        "# Notebook Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9enmd6haZyq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0mZe3YjTgkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "import pydot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VOapXXZvlHf7"
      },
      "source": [
        "# Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gwSiDD4qlHf3",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuRp5dUXuj48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataRead(fname):\n",
        "    print( \"Input File Reading\")\n",
        "    fp = open(fname, 'r')\n",
        "    samples = fp.read().strip().split('\\n\\n')\n",
        "    sent_lengths   = []\t\t#1-d array\n",
        "    sent_contents  = []\t\t#2-d array [[w1,w2,....] ...]\n",
        "    sent_lables    = []\t\t#1-d array\n",
        "    entity1_list   = []\t\t#2-d array [[e1,e1_t] [e1,e1_t]...]\n",
        "    entity2_list   = []\t\t#2-d array [[e1,e1_t] [e1,e1_t]...]\n",
        "    for sample in samples:\n",
        "      sent, entities, relation = sample.strip().split('\\n')\n",
        "\n",
        "      e1, e1_t, e2, e2_t = entities.split('\\t') \n",
        "      sent_contents.append(sent.lower())\n",
        "      entity1_list.append([e1, e1_t])\n",
        "      entity2_list.append([e2, e2_t])\n",
        "      sent_lables.append(relation)\n",
        "\n",
        "    return sent_contents, entity1_list, entity2_list, sent_lables \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BlCTbE5-lHfv",
        "colab": {}
      },
      "source": [
        "def preProcess(sent):\n",
        "\tsent = sent.lower()\n",
        "\tsent = sent.replace('/',' ')\n",
        "\tsent = sent.replace('.','')\n",
        "\t\n",
        "\tsent = tokenizer.tokenize(sent)\n",
        "\tsent = ' '.join(sent)\n",
        "\tsent = re.sub('\\d', 'dg',sent)\n",
        "\treturn sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUTj_woHlHfn",
        "colab": {}
      },
      "source": [
        "def makeFeatures(sent_list, entity1_list, entity2_list):\n",
        "\tprint ('Making Features')\n",
        "\tword_list = []\n",
        "\td1_list = []\n",
        "\td2_list = []\n",
        "\ttype_list = []\n",
        "\tfor sent, ent1, ent2 in zip(sent_list, entity1_list, entity2_list):\n",
        "\t\tsent = preProcess(sent)\n",
        "\t\tsent_list1 = sent.split()\n",
        " \t\t\n",
        "\t\tentity1 = preProcess(ent1[0]).split()\n",
        "\t\tentity2 = preProcess(ent2[0]).split()\n",
        "\t\ts1 = sent_list1.index('druga')\n",
        "\t\ts2 = sent_list1.index('drugb') \n",
        "\t\t# distance1 feature\t\n",
        "\t\td1 = []\n",
        "\t\tfor i in range(len(sent_list1)):\n",
        "\t\t    if i < s1 :\n",
        "\t\t\t     d1.append(str(i - s1))\n",
        "\t\t    elif i > s1 :\n",
        "\t\t\t     d1.append(str(i - s1 ))\n",
        "\t\t    else:\n",
        "\t\t\t     d1.append('0')\n",
        "\t\t#distance2 feature\t\t\n",
        "\t\td2 = []\n",
        "\t\tfor i in range(len(sent_list1)):\n",
        "\t\t    if i < s2:\n",
        "\t\t\t     d2.append(str(i - s2))\n",
        "\t\t    elif i > s2:\n",
        "\t\t\t     d2.append(str(i - s2))\n",
        "\t\t    else:\n",
        "\t\t\t     d2.append('0')\n",
        " \n",
        "\n",
        "\treturn word_list, d1_list, d2_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wXOmLalulHfi",
        "colab": {}
      },
      "source": [
        "def findSentLengths(tr_te_list):\n",
        "\tlis = []\n",
        "\tfor lists in tr_te_list:\n",
        "\t\tlis.append([len(l) for l in lists])\n",
        "\treturn lis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kv_XMEg4lHfe",
        "colab": {}
      },
      "source": [
        "def makeWordList(lista):  \n",
        "  sent_list = sum(lista, [])\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "\n",
        "  wl = []\t\n",
        "  i = 1\n",
        "\n",
        "  wl.append('<pad>')\n",
        "  wl.append('<unkown>')\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  return wl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-8JxPmLlHfQ",
        "colab": {}
      },
      "source": [
        "def makeDistanceList(lista):\n",
        "  sent_list = sum(lista, [])\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "   \n",
        "  wl = []\t\n",
        "  i = 1\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  return wl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GLHT6yBblHfG",
        "colab": {}
      },
      "source": [
        "def readWordEmb(word_list, fname, embSize):\n",
        "  print (\"Reading word vectors\")\n",
        "  wv = []\n",
        "  wl = []\n",
        "  \n",
        "  for word in fname.vocab:\n",
        "    wv.append(fname[word])\n",
        "    wl.append(word)\n",
        "  wordemb = []\n",
        "  count = 0\n",
        "  for word in word_list:\n",
        "    if word in wl:\n",
        "      wordemb.append(wv[wl.index(word)])\n",
        "    else:\n",
        "      print(word)\n",
        "      count += 1\n",
        "      wordemb.append(np.random.rand(embSize))\n",
        "      \n",
        "  wordemb[word_list.index('<pad>')] = np.zeros(embSize)\n",
        "  wordemb = np.asarray(wordemb, dtype='object')\n",
        "      \n",
        "  print (\"number of unknown word in word embedding\", count)\n",
        "  return wordemb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "toLi7ItMlHe6",
        "colab": {}
      },
      "source": [
        "def mapWordToId(sent_contents, word_list):\n",
        "  T = []\n",
        "  for sent in sent_contents:\n",
        "    t = []\n",
        "    for w in sent:\n",
        "      t.append(word_list.index(w))\n",
        "    T.append(t)\n",
        "  return T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOx0ij5HlHet",
        "colab": {}
      },
      "source": [
        "def mapLabelToId(sent_lables, label_dict):\n",
        "  if len(label_dict) > 2:\n",
        "    return [label_dict[label] for label in sent_lables]\n",
        "  else:\n",
        "    return [int (label != 'false') for label in sent_lables]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DIlh2hNllHeZ",
        "colab": {}
      },
      "source": [
        "def paddData(listL, maxl): #W_batch, d1_tatch, d2_batch, t_batch)\n",
        "  rlist = []\n",
        "  import keras\n",
        "  for mat in listL:\n",
        "    mat_n=keras.preprocessing.sequence.pad_sequences(mat, maxlen=maxl, dtype='int32', padding='post', truncating='post', value=0.0)\n",
        "    rlist.append(np.array(mat_n))\n",
        "  return rlist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dCpQNw5vH8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "def frequentWord(sents):\n",
        "  wf = {}\n",
        "  for s in sents:\n",
        "    for w in s:\n",
        "      if w in wf:\n",
        "        wf[w]+=1\n",
        "      else:\n",
        "        wf[w]=0\n",
        "\n",
        "  sorted_x = sorted(wf.items(), key=operator.itemgetter(1),reverse=True)\n",
        "  return sorted_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj_ZIT7amZWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import networkx as nx\n",
        "import spacy\n",
        "from nltk import Tree\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ZOZPpQM_Db",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shortest_dependency_path(sents, e1=None, e2=None):\n",
        "    temp=[]\n",
        "    for s in sents:\n",
        "      \n",
        "      doc = nlp(s)\n",
        "      edges = []\n",
        "      for token in doc:\n",
        "          for child in token.children:\n",
        "              edges.append(('{0}'.format(token),\n",
        "                            '{0}'.format(child)))\n",
        "      graph = nx.Graph(edges)\n",
        "      try:\n",
        "          shortest_path = nx.shortest_path(graph, source=e1, target=e2)\n",
        "      except:\n",
        "          shortest_path = []\n",
        "      temp.append(shortest_path)\n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XunTpIymnxT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# doc = nlp(q)\n",
        "\n",
        "# def to_nltk_tree(node):\n",
        "#     if node.n_lefts + node.n_rights > 0:\n",
        "#         return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
        "#     else:\n",
        "#         return node.orth_\n",
        "\n",
        "\n",
        "# [to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji4LATYbnm4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nx.shortest_path(graph, source=e1, target=e2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWh2CMp_CPTI",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEHiGGeq_EjU",
        "colab_type": "text"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWhiMtuVtyPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas  as pd\n",
        "import sklearn as sk\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZhjtveIxvCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWxQLpRfvLrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_embSize = 200\n",
        "d1_emb_size=10\n",
        "d2_emb_size=10\n",
        "type_emb_size=10\n",
        "numfilter = 200\n",
        "LSTM_unit = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKuNiBAxvqm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 18\n",
        "batch_size=200\n",
        "reg_para = 0.001\n",
        "drop_out = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_3DXsxB_SI7",
        "colab_type": "text"
      },
      "source": [
        "## Read FIles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iSmj2MtwCkV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ftrain = r'train_data.txt'\n",
        "ftest = r'test_data.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyHoEvsW2SX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tr_sent_contents, Tr_entity1_list, Tr_entity2_list, Tr_sent_lables = dataRead(ftrain)\n",
        "Te_sent_contents, Te_entity1_list, Te_entity2_list, Te_sent_lables = dataRead(ftest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sm1zAz1SBnj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.DataFrame({'sents':Tr_sent_contents})\n",
        "freq = pd.Series(' '.join(train['sents']).split()).value_counts()[-600:]\n",
        "# freq\n",
        "freq = list(freq.index)\n",
        "train['sents'] = train['sents'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "train['sents'].head()\n",
        "\n",
        "\n",
        "\n",
        "test=pd.DataFrame({'sents':Te_sent_contents})\n",
        "# \n",
        "\n",
        "freq = pd.Series(' '.join(test['sents']).split()).value_counts()[-300:]\n",
        "# freq\n",
        "freq = list(freq.index)\n",
        "test['sents'] = test['sents'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "test['sents'].head()\n",
        "\n",
        "\n",
        "Tr_sent_contents=list(train['sents'])\n",
        "Te_sent_contents=list(test['sents'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMT5hKdThGPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Tr_sent_contents_SDP=shortest_dependency_path(Tr_sent_contents, e1='druga', e2='drugb')\n",
        "Te_sent_contents_SDP=shortest_dependency_path(Te_sent_contents, e1='druga', e2='drugb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YhvxAhqDFCV",
        "colab_type": "text"
      },
      "source": [
        "## Normal Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxXFHuTYDoKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tr_word_list, Tr_d1_list, Tr_d2_list = makeFeatures(Tr_sent_contents, Tr_entity1_list, Tr_entity2_list)\n",
        "                                                                            \n",
        "                                                                            \n",
        "\n",
        "Tr_word_pos_t=[]\n",
        "for i in Tr_word_list:\n",
        "  Tr_word_pos_t.append(nltk.pos_tag(i))\n",
        "\n",
        "Tr_word_pos=[]\n",
        "for i in range(np.shape(Tr_word_pos_t)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Tr_word_pos_t[i])[0]):\n",
        "    temp.append(Tr_word_pos_t[i][j][1])\n",
        "  Tr_word_pos.append(temp)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBgDrhPFG_6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Te_word_list, Te_d1_list, Te_d2_list = makeFeatures(Te_sent_contents, Te_entity1_list, Te_entity2_list)                                \n",
        "\n",
        "Te_word_pos_t=[]\n",
        "for i in Te_word_list:\n",
        "  Te_word_pos_t.append(nltk.pos_tag(i))\n",
        "\n",
        "Te_word_pos=[]\n",
        "for i in range(np.shape(Te_word_pos_t)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Te_word_pos_t[i])[0]):\n",
        "    temp.append(Te_word_pos_t[i][j][1])\n",
        "  Te_word_pos.append(temp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7LJSN3BrWgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dict = makeWordList([Tr_word_list, Te_word_list])\n",
        "print (\"word dictonary length\", len(word_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCqp-tkxDQIH",
        "colab_type": "text"
      },
      "source": [
        "## SDP Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbGfeFrdruLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Tr_sent_contents_SDP_expand=[]\n",
        "\n",
        "for i in range(np.shape(Tr_word_list)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Tr_word_list[i])[0]):\n",
        "    \n",
        "    if Tr_word_list[i][j] in Tr_sent_contents_SDP[i]:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  \n",
        "  Tr_sent_contents_SDP_expand.append(temp)    \n",
        "  \n",
        "  \n",
        "  \n",
        "Te_sent_contents_SDP_expand=[]\n",
        "\n",
        "for i in range(np.shape(Te_word_list)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Te_word_list[i])[0]):\n",
        "    \n",
        "    if Te_word_list[i][j] in Te_sent_contents_SDP[i]:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  \n",
        "  Te_sent_contents_SDP_expand.append(temp)    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObBah5wWnbFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epAbIfcLDbFw",
        "colab_type": "text"
      },
      "source": [
        "## Build Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSgygYxvKg9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sent_lengths,test_sent_lengths = findSentLengths([Tr_word_list,Te_word_list])\n",
        "sentMax = max(train_sent_lengths  + test_sent_lengths)\n",
        "print (\"max sent length\", sentMax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q24TlISp8NDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max(test_sent_lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED2x_GxBBgC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentMax=105"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soy6jXmWKsYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sent_lengths = np.array(train_sent_lengths, dtype='int32')\n",
        "test_sent_lengths = np.array(test_sent_lengths, dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC2EMnCXKz7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict = {'false':0, 'advise': 1, 'mechanism': 2, 'effect': 3, 'int': 4}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92J213vzK16Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dict = makeWordList([Tr_word_list, Te_word_list])\n",
        "d1_dict = makeDistanceList([Tr_d1_list,  Te_d1_list,])\n",
        "d2_dict = makeDistanceList([Tr_d2_list,  Te_d2_list])\n",
        "pos_dict=makeDistanceList([Tr_word_pos,Te_word_pos])\n",
        "\n",
        "print (\"word dictonary length\", len(word_dict))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNNqyJ8VFf_z",
        "colab_type": "text"
      },
      "source": [
        "## Build Word Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxdFrSm8BXQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gensim\n",
        "from gensim.models import KeyedVectors\n",
        "filename = r'drive/My Drive/Colab Notebooks/embeddings/wikipedia-pubmed-and-PMC-w2v.bin'\n",
        "WV_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBP8YnB1MOkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # # Word Embedding\n",
        "wv = readWordEmb(word_dict, WV_model, wv_embSize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-f7_U-GFpAj",
        "colab_type": "text"
      },
      "source": [
        "## Mapping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1KLagN_OcAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mapping Train\n",
        "W_train =   mapWordToId(Tr_word_list, word_dict)\n",
        "d1_train = mapWordToId(Tr_d1_list, d1_dict)\n",
        "d2_train = mapWordToId(Tr_d2_list, d2_dict)\n",
        "pos_train=mapWordToId(Tr_word_pos, pos_dict)\n",
        "\n",
        "#One Hot Encoding\n",
        "Y_t = mapLabelToId(Tr_sent_lables, label_dict)\n",
        "Y_train = np.zeros((len(Y_t), len(label_dict)))\n",
        "for i in range(len(Y_t)):\n",
        "\tY_train[i][Y_t[i]] = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK9IzWyMPJyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!# Mapping Test\n",
        "W_test =   mapWordToId(Te_word_list, word_dict)\n",
        "d1_test = mapWordToId(Te_d1_list, d1_dict)\n",
        "d2_test = mapWordToId(Te_d2_list, d2_dict)\n",
        "pos_test=mapWordToId(Te_word_pos, pos_dict)\n",
        "\n",
        "\n",
        "Y_t = mapLabelToId(Te_sent_lables, label_dict)\n",
        "Y_test = np.zeros((len(Y_t), len(label_dict)))\n",
        "for i in range(len(Y_t)):\n",
        "\tY_test[i][Y_t[i]] = 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R_DhZJvDq1v",
        "colab_type": "text"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JApaXpNyyli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTiEOW1_S4Ym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "da=np.zeros(sentMax)\n",
        "db=np.zeros(sentMax)\n",
        "da[0]=4\n",
        "db[0]=8\n",
        "da=np.reshape(da,(1,sentMax))\n",
        "db=np.reshape(db,(1,sentMax))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3nFk2uhPF-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#padding\n",
        "W_train, d1_train, d2_train,pos_train,T_train,Tr_sent_contents_SDP_expand,W_test, d1_test, d2_test,pos_test,Te_sent_contents_SDP_expand,T_test\\\n",
        "=paddData([W_train, d1_train, d2_train,pos_train,T_train,Tr_sent_contents_SDP_expand,W_test, d1_test, d2_test,pos_test,Te_sent_contents_SDP_expand,T_test],\n",
        "          sentMax) \n",
        "            \n",
        "print (\"train\", len(W_train))\n",
        "print (\"test\", len(W_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfnntJ43QS5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#vocabulary size\n",
        "word_dict_size = len(wv)\n",
        "d1_dict_size = len(d1_dict)\n",
        "d2_dict_size = len(d2_dict)\n",
        "pos_dict_size = len(pos_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "584wYDJBrHtq",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhhwrquM-953",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76W2TKdsGcQs",
        "colab_type": "text"
      },
      "source": [
        "### F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tqfY3bffCpUH",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H-jf9sD0jjvm"
      },
      "source": [
        "### AttentionWitPositionAndSimilarity_v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RY0EWc9njjvp",
        "colab": {}
      },
      "source": [
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AttentionWitPositionAndSimilarity_v2(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=regularizers.l2(0.0001), b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=True,\n",
        "                 **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWitPositionAndSimilarity_v2, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        \n",
        "\n",
        "        \n",
        "        self.W = self.add_weight((input_shape[0][-1],input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "       \n",
        "        \n",
        "        self.WP1 = self.add_weight((10,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_WP1'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "\n",
        "        self.WP2 = self.add_weight((10,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_WP2'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        \n",
        "        self.WS = self.add_weight((1,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_WS'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)          \n",
        "        \n",
        "        self.v = self.add_weight((input_shape[0][-1],1),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_v'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)                                 \n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[0][1],input_shape[0][-1]),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        \n",
        "        d1_emb=embedding_model.predict([da])[0][0]\n",
        "        d2_emb=embedding_model.predict([db])[0][0]\n",
        "\n",
        "        d1 =  K.variable(d1_emb, dtype='float32',name=\"input_d1\")\n",
        "        d1=K.expand_dims(d1,axis=-1)\n",
        "        d1_sco=K.dot(x1,d1)\n",
        "        d1_sco=d1_sco/wv_embSize\n",
        "        d1_sco_soft=keras.activations.softmax(d1_sco, axis=1)\n",
        "\n",
        "        d2 =  K.variable(d2_emb, dtype='float32',name=\"input_d2\")\n",
        "        d2=K.expand_dims(d2,axis=-1)\n",
        "        d2_sco=K.dot(x1,d2)\n",
        "        d2_sco=d2_sco/wv_embSize\n",
        "        d2_sco_soft=keras.activations.softmax(d2_sco, axis=1)\n",
        "\n",
        "        avg=(d1_sco+d2_sco)/2#(?,143,1)\n",
        "        \n",
        "\n",
        "        \n",
        "        ew = K.dot(x[0], self.W)\n",
        "        ewp1=K.dot(x[1],self.WP1)\n",
        "        ewp2=K.dot(x[2],self.WP2)\n",
        "        ews=K.dot(avg,self.WS)\n",
        "        \n",
        "\n",
        "      \n",
        "        avg_all=(ewp1+ewp2+ews)/3\n",
        "        avg_all=keras.activations.softmax(avg_all, axis=1)\n",
        "        eij=ew+avg_all\n",
        "        eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "        eij=K.dot(eij,self.v)\n",
        "        eij=K.squeeze(eij,axis=-1)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x[0] * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0][0], input_shape[0][-1]),\n",
        "                    (input_shape[0][0], input_shape[0][1])]\n",
        "        else:\n",
        "            return input_shape[0][0], input_shape[0][-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xg3lTZABeSpU"
      },
      "source": [
        "### similarityAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aoJLPzy2eSpj",
        "colab": {}
      },
      "source": [
        "##no softmax\n",
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class similarityAttention(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=regularizers.l2(0.0001), b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=True,\n",
        "                 **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(similarityAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        \n",
        "\n",
        "        \n",
        "        self.W = self.add_weight((input_shape[-1],input_shape[-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "       \n",
        "        \n",
        "        \n",
        "        self.WS = self.add_weight((1,input_shape[-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_WS'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)          \n",
        "        \n",
        "        self.v = self.add_weight((input_shape[-1],1),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_v'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)                                 \n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],input_shape[-1]),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        \n",
        "        d1_emb=embedding_model.predict([da])[0][0]\n",
        "        d2_emb=embedding_model.predict([db])[0][0]\n",
        "\n",
        "        d1 =  K.variable(d1_emb, dtype='float32',name=\"input_d1\")\n",
        "        d1=K.expand_dims(d1,axis=-1)\n",
        "        d1_sco=K.dot(x1,d1)\n",
        "        d1_sco=d1_sco/wv_embSize\n",
        "        d1_sco_soft=keras.activations.softmax(d1_sco, axis=1)\n",
        "\n",
        "        d2 =  K.variable(d2_emb, dtype='float32',name=\"input_d2\")\n",
        "        d2=K.expand_dims(d2,axis=-1)\n",
        "        d2_sco=K.dot(x1,d2)\n",
        "        d2_sco=d2_sco/wv_embSize\n",
        "        d2_sco_soft=keras.activations.softmax(d2_sco, axis=1)\n",
        "\n",
        "        avg=(d1_sco+d2_sco)/2#(?,143,1)\n",
        "        \n",
        "\n",
        "        \n",
        "        ew = K.dot(x, self.W)\n",
        "        ews=K.dot(avg,self.WS)\n",
        "        \n",
        "\n",
        "        avg_all=keras.activations.softmax(ews, axis=1)\n",
        "        eij=ew+avg_all\n",
        "        eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "        eij=K.dot(eij,self.v)\n",
        "        eij=K.squeeze(eij,axis=-1)\n",
        "#         K.squeeze(eig,axis=-1)\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], input_shape[-1]),\n",
        "                    (input_shape[0], input_shape[1])]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nte_xassIpSD",
        "colab_type": "text"
      },
      "source": [
        "### TemporalMaxPooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gm-msX4Itdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TemporalMaxPooling\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec\n",
        "from keras.engine.topology import Layer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TemporalMaxPooling(Layer):\n",
        "    \"\"\"\n",
        "    This pooling layer accepts the temporal sequence output by a recurrent layer\n",
        "    and performs temporal pooling, looking at only the non-masked portion of the sequence.\n",
        "    The pooling layer converts the entire variable-length hidden vector sequence\n",
        "    into a single hidden vector.\n",
        "    Modified from https://github.com/fchollet/keras/issues/2151 so code also\n",
        "    works on tensorflow backend. Updated syntax to match Keras 2.0 spec.\n",
        "    Args:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "        input shape: (nb_samples, nb_timesteps, nb_features)\n",
        "        output shape: (nb_samples, nb_features)\n",
        "    Examples:\n",
        "        > x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
        "        > x = TemporalMaxPooling()(x)\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(TemporalMaxPooling, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.input_spec = InputSpec(ndim=3)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[2])\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        if mask is None:\n",
        "            mask = K.sum(K.ones_like(x), axis=-1)\n",
        "\n",
        "        # if masked, set to large negative value so we ignore it when taking max of the sequence\n",
        "        # K.switch with tensorflow backend is less useful than Theano's\n",
        "        if K._BACKEND == 'tensorflow':\n",
        "            mask = K.expand_dims(mask, axis=-1)\n",
        "            mask = K.tile(mask, (1, 1, K.int_shape(x)[2]))\n",
        "            masked_data = K.tf.where(K.equal(mask, K.zeros_like(mask)),\n",
        "                K.ones_like(x)*-np.inf, x)  # if masked assume value is -inf\n",
        "            return K.max(masked_data, axis=1)\n",
        "        else:  # theano backend\n",
        "            mask = mask.dimshuffle(0, 1, \"x\")\n",
        "            masked_data = K.switch(K.eq(mask, 0), -np.inf, x)\n",
        "            return masked_data.max(axis=1)\n",
        "\n",
        "    def compute_mask(self, input, mask):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1STMh_VI2o2",
        "colab_type": "text"
      },
      "source": [
        "### Attentive Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YqOIql-I1kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        # todo: check that this is correct\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=True,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Note: The layer has been tested with Keras 1.x\n",
        "        Example:\n",
        "        \n",
        "            # 1\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "            # 2 - Get the attention scores\n",
        "            hidden = LSTM(64, return_sequences=True)(words)\n",
        "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        eij = dot_product(x, self.W)\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0], input_shape[-1]),\n",
        "                    (input_shape[0], input_shape[1])]\n",
        "        else:\n",
        "            return input_shape[0], input_shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyQPX0ujI6WN",
        "colab_type": "text"
      },
      "source": [
        "### AttentionWitPosition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4u5wYygI-5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "def dot_product(x, kernel):\n",
        "    \"\"\"\n",
        "    Wrapper for dot product operation, in order to be compatible with both\n",
        "    Theano and Tensorflow\n",
        "    Args:\n",
        "        x (): input\n",
        "        kernel (): weights\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    if K.backend() == 'tensorflow':\n",
        "        # todo: check that this is correct\n",
        "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
        "    else:\n",
        "        return K.dot(x, kernel)\n",
        "\n",
        "\n",
        "class AttentionWitPosition(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=regularizers.l2(0.0001), b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=False,\n",
        "                 **kwargs):\n",
        "        \n",
        "       \n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWitPosition, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        \n",
        "\n",
        "        \n",
        "        self.W = self.add_weight((input_shape[0][-1],input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "       \n",
        "        \n",
        "        self.WP1 = self.add_weight((10,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_WP1'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "\n",
        "        self.WP2 = self.add_weight((10,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_WP2'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        \n",
        "      \n",
        "        \n",
        "        self.v = self.add_weight((input_shape[0][-1],1),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_v'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)                                 \n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[0][1],input_shape[0][-1]),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "      \n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        ew = K.dot(x[0], self.W)\n",
        "        ewp1=K.dot(x[1],self.WP1)\n",
        "        ewp2=K.dot(x[2],self.WP2)\n",
        "        eij=ew+ewp1+ewp2\n",
        "        eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "        eij=K.dot(eij,self.v)\n",
        "        eij=K.squeeze(eij,axis=-1)\n",
        "#         K.squeeze(eig,axis=-1)\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x[0] * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0][0], input_shape[0][-1]),\n",
        "                    (input_shape[0][0], input_shape[0][1])]\n",
        "        else:\n",
        "            return input_shape[0][0], input_shape[0][-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfKUFNGw_LEw",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-HeIqbArPn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential \n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import *\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgOfB4G4SvtN",
        "colab_type": "text"
      },
      "source": [
        "#### Multi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYJCS3434qG_",
        "colab_type": "text"
      },
      "source": [
        "#####Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p1xeU4aGDFL0",
        "colab": {}
      },
      "source": [
        "#main\n",
        "K.clear_session()\n",
        "\n",
        "input1=Input(shape=(sentMax,),name='text')\n",
        "input2=Input(shape=(sentMax,),name='d1')\n",
        "input3=Input(shape=(sentMax,),name='d2')\n",
        "input4=Input(shape=(sentMax,),name='pos')\n",
        "input5=Input(shape=(sentMax,),name='SDP_main')\n",
        "\n",
        "input5_r=Reshape((sentMax,1))(input5)\n",
        "\n",
        "x1=Embedding(np.shape(wv)[0], 200, weights=[wv], input_length=sentMax,trainable=True)(input1)\n",
        "embedding_model=Model(inputs=[input1], outputs=x1)\n",
        "x2=Embedding(d1_dict_size, 10,trainable=True,input_length=sentMax)(input2)\n",
        "x3=Embedding(d2_dict_size, 10,trainable=True,input_length=sentMax)(input3)\n",
        "x4=Embedding(pos_dict_size, 4,trainable=True,input_length=sentMax)(input4)\n",
        "# x5=Dense((sentMax,1))(input5)\n",
        "inputs=concatenate([x1,x2,x3,x4,input5_r],axis=-1,name=\"concat\")\n",
        "inputs=BatchNormalization()(inputs)\n",
        "inputs=SpatialDropout1D(0.4)(inputs)\n",
        "model_h1=Bidirectional(CuDNNLSTM(200,return_sequences=True),merge_mode='concat')(inputs)\n",
        "\n",
        "att,a=AttentionWitPositionAndSimilarity_v2()([model_h1,x2,x3])\n",
        "\n",
        "att=BatchNormalization()(att)\n",
        "main_output=Dropout(0.5, noise_shape=None, seed=None)(att)   \n",
        "\n",
        "main_output=Dense(300,kernel_regularizer=regularizers.l2(0.001))(main_output)            \n",
        "main_output=BatchNormalization()(main_output)\n",
        "main_output=LeakyReLU()(main_output)\n",
        "\n",
        "main_output=Dropout(0.5, noise_shape=None, seed=None)(main_output)  \n",
        "main_output = Dense(100,kernel_regularizer=regularizers.l2(0.001))(main_output)\n",
        "main_output=BatchNormalization()(main_output)\n",
        "main_output=LeakyReLU()(main_output)\n",
        "\n",
        "main_output = Dense(5,activation='softmax',kernel_regularizer=regularizers.l2(0.0001))(main_output)\n",
        "\n",
        "# ,input1_h1,input2_h1,input3_h1,input4_h1,input_sdp\n",
        "output_model = Model(inputs=[input1,input2,input3,input4,input5], outputs=main_output)\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
        "# adam=Adam(lr=0.001,  beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
        "output_model.compile(loss = 'categorical_crossentropy', optimizer=sgd,metrics=[f1])\n",
        "##78.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ4a0xuh1okY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uhYZzJxBboLA",
        "colab": {}
      },
      "source": [
        "from keras.utils.vis_utils import plot_model as plot\n",
        "plot(output_model, to_file='./model.png', show_shapes=True)\n",
        "from IPython.display import Image\n",
        "Image('./model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK5JLoXC46BT",
        "colab_type": "text"
      },
      "source": [
        "#####Prepare to fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STO64e3Ez8v6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "yt=Y_train.copy()\n",
        "yt=np.argmax(Y_train,axis=-1)\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(yt),\n",
        "                                                 yt)\n",
        "print(class_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf8qWdySc3sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "te_key_word_vec=np.asarray(te_key_word_vec)\n",
        "tr_key_word_vec=np.asarray(tr_key_word_vec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8HaR9mBboLG",
        "colab": {}
      },
      "source": [
        "mcp_save = ModelCheckpoint('mdl_wts_m_{epoch:02d}.hdf5', save_best_only=False, monitor='val_loss', mode='min')\n",
        "mcp_save2 = ModelCheckpoint('mdl_wts_m2_{epoch:02d}.hdf5', save_best_only=False, monitor='val_loss', mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j9L09uN6H75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0yLIqGB5dsX",
        "colab_type": "text"
      },
      "source": [
        "#####Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPphWdWSK-r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " output_model.fit([W_train,d1_train,d2_train,pos_train,Tr_sent_contents_SDP_expand], Y_train,\n",
        "                 epochs = 300, shuffle=True, batch_size=200, verbose = 1,\n",
        "                 validation_data=([W_test, d1_test,d2_test,pos_test,Te_sent_contents_SDP_expand],Y_test),\n",
        "                 callbacks=[mcp_save2],class_weight=class_weights)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifnd5FWA50Z_",
        "colab_type": "text"
      },
      "source": [
        "##### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc0hq_Vhszvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1,10):\n",
        "  output_model.load_weights(filepath = 'mdl_wts_m2_0'+str(i)+'.hdf5')\n",
        "  from sklearn import metrics\n",
        "  y_prob_m=output_model.predict([W_test, d1_test,d2_test,pos_test,Te_sent_contents_SDP_expand])\n",
        "  y_prob_m=np.argmax(y_prob_m,axis=-1)\n",
        "  yt=Y_test.copy()\n",
        "  yt=np.argmax(Y_test,axis=-1)\n",
        "  matrix = metrics.confusion_matrix(yt, y_prob_m)\n",
        "  print(str(i)+': '+str(f1_score(yt,y_prob_m,[1,2,3,4],average='micro')))\n",
        "  \n",
        "  \n",
        "for i in range(10,301):\n",
        "  output_model.load_weights(filepath = 'mdl_wts_m2_'+str(i)+'.hdf5')\n",
        "  from sklearn import metrics\n",
        "  y_prob_m=output_model.predict([W_test, d1_test,d2_test,pos_test,Te_sent_contents_SDP_expand])\n",
        "  y_prob_m=np.argmax(y_prob_m,axis=-1)\n",
        "  yt=Y_test.copy()\n",
        "  yt=np.argmax(Y_test,axis=-1)\n",
        "  matrix = metrics.confusion_matrix(yt, y_prob_m)\n",
        "  print(str(i)+': '+str(f1_score(yt,y_prob_m,[1,2,3,4],average='micro')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFpCWfNHewsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the best model\n",
        "output_model.load_weights(filepath = 'mdl_wts_m2_1.hdf5')\n",
        "y_prob_m=output_model.predict([W_test, d1_test,d2_test,pos_test,Te_sent_contents_SDP_expand])\n",
        "y_prob_m=np.argmax(y_prob_m,axis=-1)\n",
        "yt=Y_test.copy()\n",
        "yt=np.argmax(Y_test,axis=-1)\n",
        "matrix = metrics.confusion_matrix(yt, y_prob_m)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UTlR3xjUboL-",
        "colab": {}
      },
      "source": [
        "matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RDGVDSVkboME",
        "colab": {}
      },
      "source": [
        "f1_score(yt,y_prob_m,[1,2,3,4],average='micro'),precision_score(yt,y_prob_m,average='micro'),recall_score(yt,y_prob_m,average='micro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z0KpZVwtboMK",
        "colab": {}
      },
      "source": [
        "f1_score(yt,y_prob_m,average=None),precision_score(yt,y_prob_m,average=None),recall_score(yt,y_prob_m,average=None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}